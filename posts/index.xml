<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Portfolio</title>
    <link>//localhost:1313/posts/</link>
    <description>Recent content in Posts on Portfolio</description>
    <image>
      <title>Portfolio</title>
      <url>//localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>//localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.143.1</generator>
    <language>en</language>
    <atom:link href="//localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers</title>
      <link>//localhost:1313/posts/prc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/prc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;&lt;span style=&#34;font-size: 20px;&#34;&gt;OJSP, ICASSP 2025&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Masahiro Kada&lt;sup&gt;1&lt;/sup&gt;  Ryota Yoshihashi&lt;sup&gt;1&lt;/sup&gt;  Satoshi Ikehata&lt;sup&gt;1, 2&lt;/sup&gt;  Rei Kawakami&lt;sup&gt;1&lt;/sup&gt;  Ikuro Sato&lt;sup&gt;1, 3&lt;/sup&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 15px&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;Institute of Science Tokyo  &lt;sup&gt;2&lt;/sup&gt;National Institute of Informatics  &lt;sup&gt;3&lt;/sup&gt;Denso IT Laboratory&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10858379&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/m82kada/PRC/blob/main/README.md?plain=1&#34;&gt;Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;PRC&#34; loading=&#34;lazy&#34; src=&#34;.image/demo.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This repository contains the code for Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers (PRC). This work has been published in &lt;a href=&#34;https://ieeexplore.ieee.org/document/10858379&#34;&gt;OJSP&lt;/a&gt; and will be presented at ICASSP 2025.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href=&#34;&#34;&gt;10.1109/OJSP.2025.3536853&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models
inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
