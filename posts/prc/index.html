<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers | Portfolio</title>
<meta name=keywords content="markdown,syntax,code,gist"><meta name=description content="OJSP, ICASSP 2025
Masahiro Kada1  Ryota Yoshihashi1  Satoshi Ikehata1, 2  Rei Kawakami1  Ikuro Sato1, 3
1Institute of Science Tokyo  2National Institute of Informatics  3Denso IT Laboratory
Link:  Paper  |  Code

Abstract
Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models
inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations."><meta name=author content><link rel=canonical href=//localhost:1313/posts/prc/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=//localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=//localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=//localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=//localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=//localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=//localhost:1313/posts/prc/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="//localhost:1313/posts/prc/"><meta property="og:site_name" content="Portfolio"><meta property="og:title" content="Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers"><meta property="og:description" content="OJSP, ICASSP 2025
Masahiro Kada1 Ryota Yoshihashi1 Satoshi Ikehata1, 2 Rei Kawakami1 Ikuro Sato1, 3
1Institute of Science Tokyo 2National Institute of Informatics 3Denso IT Laboratory
Link: Paper | Code
Abstract Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:tag" content="Markdown"><meta property="article:tag" content="Syntax"><meta property="article:tag" content="Code"><meta property="article:tag" content="Gist"><meta property="og:image" content="//localhost:1313/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="//localhost:1313/%3Cimage%20path/url%3E"><meta name=twitter:title content="Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers"><meta name=twitter:description content="OJSP, ICASSP 2025
Masahiro Kada1  Ryota Yoshihashi1  Satoshi Ikehata1, 2  Rei Kawakami1  Ikuro Sato1, 3
1Institute of Science Tokyo  2National Institute of Informatics  3Denso IT Laboratory
Link:  Paper  |  Code

Abstract
Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models
inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"//localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers","item":"//localhost:1313/posts/prc/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers","name":"Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers","description":"OJSP, ICASSP 2025\nMasahiro Kada1 Ryota Yoshihashi1 Satoshi Ikehata1, 2 Rei Kawakami1 Ikuro Sato1, 3\n1Institute of Science Tokyo 2National Institute of Informatics 3Denso IT Laboratory\nLink: Paper | Code\nAbstract Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations.\n","keywords":["markdown","syntax","code","gist"],"articleBody":"OJSP, ICASSP 2025\nMasahiro Kada1 Ryota Yoshihashi1 Satoshi Ikehata1, 2 Rei Kawakami1 Ikuro Sato1, 3\n1Institute of Science Tokyo 2National Institute of Informatics 3Denso IT Laboratory\nLink: Paper | Code\nAbstract Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations.\nEvaluation BibTeX @ARTICLE{10858379, author={Kada, Masahiro and Yoshihashi, Ryota and Ikehata, Satoshi and Kawakami, Rei and Sato, Ikuro}, journal={IEEE Open Journal of Signal Processing}, title={Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers}, year={2025}, volume={}, number={}, pages={1-9}, keywords={Perturbation methods;Routing;Transformers;Predictive models;Contrastive learning;Data models;Computational modeling;Training;Image classification;Computer vision;Mixture of Experts;Dynamic Neural Network;Image Classification;Vision Transformer}, doi={10.1109/OJSP.2025.3536853}} Acknowledgements This work was supported by JSPS KAKENHI Grant Number JP22H03642 and DENSO IT LAB Recognition and Learning Algorithm Collaborative Research Chair (Science Tokyo).\n","wordCount":"246","inLanguage":"en","image":"//localhost:1313/%3Cimage%20path/url%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"//localhost:1313/posts/prc/"},"publisher":{"@type":"Organization","name":"Portfolio","logo":{"@type":"ImageObject","url":"//localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=//localhost:1313/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=//localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=//localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers</h1><div class=post-meta></div></header><div class=post-content><p><strong><span style=font-size:20px>OJSP, ICASSP 2025</span></strong></p><p><strong>Masahiro Kada<sup>1</sup>  Ryota Yoshihashi<sup>1</sup>  Satoshi Ikehata<sup>1, 2</sup>  Rei Kawakami<sup>1</sup>  Ikuro Sato<sup>1, 3</sup></strong></p><p><span style=font-size:15px><sup>1</sup>Institute of Science Tokyo  <sup>2</sup>National Institute of Informatics  <sup>3</sup>Denso IT Laboratory</span></p><p>Link:  <a href=https://ieeexplore.ieee.org/document/10858379>Paper</a>  |  <a href="https://github.com/m82kada/PRC/blob/main/README.md?plain=1">Code</a></p><p><img alt=PRC loading=lazy src=.image/demo.png></p><h2 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><p>Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models
inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations.</p><p><img alt=PRC loading=lazy src=.image/prc.png></p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><h2 id=bibtex>BibTeX<a hidden class=anchor aria-hidden=true href=#bibtex>#</a></h2><pre tabindex=0><code>@ARTICLE{10858379,
  author={Kada, Masahiro and Yoshihashi, Ryota and Ikehata, Satoshi and Kawakami, Rei and Sato, Ikuro},
  journal={IEEE Open Journal of Signal Processing}, 
  title={Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  keywords={Perturbation methods;Routing;Transformers;Predictive models;Contrastive learning;Data models;Computational modeling;Training;Image classification;Computer vision;Mixture of Experts;Dynamic Neural Network;Image Classification;Vision Transformer},
  doi={10.1109/OJSP.2025.3536853}}
</code></pre><h2 id=acknowledgements>Acknowledgements<a hidden class=anchor aria-hidden=true href=#acknowledgements>#</a></h2><p>This work was supported by JSPS KAKENHI Grant Number JP22H03642 and DENSO IT LAB Recognition and Learning Algorithm Collaborative Research Chair (Science Tokyo).</p></div><footer class=post-footer><ul class=post-tags><li><a href=//localhost:1313/tags/markdown/>Markdown</a></li><li><a href=//localhost:1313/tags/syntax/>Syntax</a></li><li><a href=//localhost:1313/tags/code/>Code</a></li><li><a href=//localhost:1313/tags/gist/>Gist</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=//localhost:1313/>Portfolio</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>