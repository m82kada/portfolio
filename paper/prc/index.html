<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers | Portfolio</title>
<meta name=keywords content><meta name=description content="OJSP, ICASSP 2025"><meta name=author content><link rel=canonical href=https://m82kada.github.io/portfolio/paper/prc/><link crossorigin=anonymous href=/portfolio/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://m82kada.github.io/portfolio/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://m82kada.github.io/portfolio/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://m82kada.github.io/portfolio/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://m82kada.github.io/portfolio/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://m82kada.github.io/portfolio/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://m82kada.github.io/portfolio/paper/prc/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://m82kada.github.io/portfolio/paper/prc/"><meta property="og:site_name" content="Portfolio"><meta property="og:title" content="Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers"><meta property="og:description" content="OJSP, ICASSP 2025"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="paper"><meta property="og:image" content="https://m82kada.github.io/portfolio/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://m82kada.github.io/portfolio/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers"><meta name=twitter:description content="OJSP, ICASSP 2025"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Papers","item":"https://m82kada.github.io/portfolio/paper/"},{"@type":"ListItem","position":2,"name":"Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers","item":"https://m82kada.github.io/portfolio/paper/prc/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers","name":"Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers","description":"OJSP, ICASSP 2025","keywords":[""],"articleBody":"OJSP, ICASSP 2025\nMasahiro Kada1 Ryota Yoshihashi1 Satoshi Ikehata1, 2 Rei Kawakami1 Ikuro Sato1, 3\n1Institute of Science Tokyo 2National Institute of Informatics 3Denso IT Laboratory\nLink: Paper | Code\nAbstract Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations. Method We trained V-MoE [C. Riquelme+, NeurIPS 2021] with Pairwise Router Consistency (PRC) as a regularization term to ensure that the same input image patches are routed consistently.\nEvaluation We conducted quantitative evaluations on the ImageNet-1K, CIFAR-10, CIFAR-100, and Oxford Flowers-102 datasets. Top-k ImageNet-1K CIFAR-10 CIFAR-100 Flowers V-MoE-S 2 75.84% 95.20% 81.38% 89.30% V-MoE-S w/ PRC 2 76.27% 95.36% 82.27% 90.18% V-MoE-S 1 75.23% 94.81% 81.18% 90.21% V-MoE-S w/ PRC 1 75.92% 95.12% 82.12% 91.24% We visualized the changes in routing as Gaussian noise was gradually added. Original Image\nV-MoE\nV-MoE w/ PRC\nNoise Level Image 1 Image 2 Image 3 Image 4 Image 5 BibTeX @ARTICLE{10858379, author={Kada, Masahiro and Yoshihashi, Ryota and Ikehata, Satoshi and Kawakami, Rei and Sato, Ikuro}, journal={IEEE Open Journal of Signal Processing}, title={Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers}, year={2025}, volume={}, number={}, pages={1-9}, keywords={Perturbation methods;Routing;Transformers;Predictive models;Contrastive learning;Data models;Computational modeling;Training;Image classification;Computer vision;Mixture of Experts;Dynamic Neural Network;Image Classification;Vision Transformer}, doi={10.1109/OJSP.2025.3536853}} Acknowledgements This work was supported by JSPS KAKENHI Grant Number JP22H03642 and DENSO IT LAB Recognition and Learning Algorithm Collaborative Research Chair (Science Tokyo).\n","wordCount":"350","inLanguage":"en","image":"https://m82kada.github.io/portfolio/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://m82kada.github.io/portfolio/paper/prc/"},"publisher":{"@type":"Organization","name":"Portfolio","logo":{"@type":"ImageObject","url":"https://m82kada.github.io/portfolio/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://m82kada.github.io/portfolio/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://m82kada.github.io/portfolio/>Home</a>&nbsp;»&nbsp;<a href=https://m82kada.github.io/portfolio/paper/>Papers</a></div><h1 class="post-title entry-hint-parent">Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers</h1><div class=post-meta></div></header><div class=post-content><p><strong><span style=font-size:20px>OJSP, ICASSP 2025</span></strong></p><p><strong>Masahiro Kada<sup>1</sup>  Ryota Yoshihashi<sup>1</sup>  Satoshi Ikehata<sup>1, 2</sup>  Rei Kawakami<sup>1</sup>  Ikuro Sato<sup>1, 3</sup></strong></p><p><span style=font-size:15px><sup>1</sup>Institute of Science Tokyo  <sup>2</sup>National Institute of Informatics  <sup>3</sup>Denso IT Laboratory</span></p><p>Link:  <a href=https://ieeexplore.ieee.org/document/10858379>Paper</a>  |  <a href="https://github.com/m82kada/PRC/blob/main/README.md?plain=1">Code</a></p><p><img alt=PRC loading=lazy src=../images/prc/demo.png></p><h2 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><p>Mixture of experts with a sparse expert selection rule has been gaining much attention recently because of its scalability without compromising inference time. However, unlike standard neural networks, sparse mixture-of-experts models
inherently exhibit discontinuities in the output space, which may impede the acquisition of appropriate invariance to the input perturbations, leading to a deterioration of model performance for tasks such as classification. To address this issue, we propose Pairwise Router Consistency (PRC) that effectively penalizes the discontinuities occurring under natural deformations of input images. With the supervised loss, the use of PRC loss empirically improves classification accuracy on ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, compared to a baseline method. Notably, our method with 1-expert selection slightly outperforms the baseline method using 2-expert selection. We also confirmed that models trained with our method experience discontinuous changes less frequently under input perturbations.<br></p><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><p>We trained V-MoE [C. Riquelme+, NeurIPS 2021] with Pairwise Router Consistency (PRC) as a regularization term to ensure that the same input image patches are routed consistently.</p><p><img alt=PRC loading=lazy src=../images/prc/prc.png><br></p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>We conducted quantitative evaluations on the ImageNet-1K, CIFAR-10, CIFAR-100, and Oxford Flowers-102 datasets.<br></p><table><thead><tr><th></th><th>Top-k</th><th>ImageNet-1K</th><th>CIFAR-10</th><th>CIFAR-100</th><th>Flowers</th></tr></thead><tbody><tr><td>V-MoE-S</td><td>2</td><td>75.84%</td><td>95.20%</td><td>81.38%</td><td>89.30%</td></tr><tr><td><strong>V-MoE-S w/ PRC</strong></td><td>2</td><td>76.27%</td><td>95.36%</td><td>82.27%</td><td>90.18%</td></tr><tr><td>V-MoE-S</td><td>1</td><td>75.23%</td><td>94.81%</td><td>81.18%</td><td>90.21%</td></tr><tr><td><strong>V-MoE-S w/ PRC</strong></td><td>1</td><td>75.92%</td><td>95.12%</td><td>82.12%</td><td>91.24%</td></tr></tbody></table><br>We visualized the changes in routing as Gaussian noise was gradually added.<div class=images-folder><div><img id=img1 src=../images/prc/4-0-0.png class=images><p id=mainImgLabel class=image-label>Original Image</p></div><div><img id=img2 src=../images/prc/4-1-0.png class=images><p class=image-label>V-MoE</p></div><div><img id=img3 src=../images/prc/4-2-0.png class=images><p class="prc-label image-label">V-MoE w/ PRC</p></div></div><div class=slider-container><label class=noise-label>Noise Level</label>
<input type=range id=slider min=0 max=9 step=1 value=0 oninput=selectNoise(this.value)></div><div class=button-group><button class="img-btn active" onclick=selectImage(0)>Image 1</button>
<button class=img-btn onclick=selectImage(1)>Image 2</button>
<button class=img-btn onclick=selectImage(2)>Image 3</button>
<button class=img-btn onclick=selectImage(3)>Image 4</button>
<button class=img-btn onclick=selectImage(4)>Image 5</button></div><script>var image_number=4,noise_level=0;function selectImage(e){image_number=4-e,document.querySelectorAll(".img-btn").forEach((t,n)=>{t.classList.toggle("active",n===e)}),updateImages()}function selectNoise(e){noise_level=e,updateImages()}function updateImages(){noise_level==0?document.getElementById("mainImgLabel").innerHTML="Original Image":document.getElementById("mainImgLabel").innerHTML="Noised Image",document.getElementById("img1").src="../images/prc/"+image_number+"-0-"+noise_level+".png",document.getElementById("img2").src="../images/prc/"+image_number+"-1-"+noise_level+".png",document.getElementById("img3").src="../images/prc/"+image_number+"-2-"+noise_level+".png"}</script><style>.button-group{display:flex;justify-content:center;align-items:center}.button-group label{font-size:16px;font-weight:700;margin-right:10px}.button-group button{padding:5px 7px;border:1px solid #888;background-color:#f0f0f0;cursor:pointer;font-size:14px;flex:1;border-radius:0;transition:background-color .3s,color .3s}.button-group button:first-child{border-radius:5px 0 0 5px}.button-group button:last-child{border-radius:0 5px 5px 0}.button-group button.active{background-color:#555;color:#fff;border-color:#333}.images-folder{margin-top:20px;display:flex;justify-content:center;gap:20px;margin-bottom:20px}.images{border-radius:0!important}.slider-container{margin-bottom:20px;display:flex;align-items:center;justify-content:center;gap:15px}input[type=range]{width:50%;-webkit-appearance:none;height:8px;background:linear-gradient(to right,#d3d3d3,#888);border-radius:5px;outline:none;opacity:.9;transition:opacity .2s}input[type=range]::-webkit-slider-thumb{-webkit-appearance:none;appearance:none;width:16px;height:16px;background:#555;border-radius:50%;cursor:pointer;box-shadow:0 0 5px rgba(0,0,0,.3)}input[type=range]::-moz-range-thumb{width:16px;height:16px;background:#555;border-radius:50%;cursor:pointer;box-shadow:0 0 5px rgba(0,0,0,.3)}input[type=range]:hover{opacity:1}.noise-label{font-weight:700;font-size:16px}.prc-label{font-weight:700}</style><br><h2 id=bibtex>BibTeX<a hidden class=anchor aria-hidden=true href=#bibtex>#</a></h2><pre tabindex=0><code>@ARTICLE{10858379,
  author={Kada, Masahiro and Yoshihashi, Ryota and Ikehata, Satoshi and Kawakami, Rei and Sato, Ikuro},
  journal={IEEE Open Journal of Signal Processing}, 
  title={Robustifying Routers Against Input Perturbations for Sparse Mixture-of-Experts Vision Transformers}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  keywords={Perturbation methods;Routing;Transformers;Predictive models;Contrastive learning;Data models;Computational modeling;Training;Image classification;Computer vision;Mixture of Experts;Dynamic Neural Network;Image Classification;Vision Transformer},
  doi={10.1109/OJSP.2025.3536853}}
</code></pre><h2 id=acknowledgements>Acknowledgements<a hidden class=anchor aria-hidden=true href=#acknowledgements>#</a></h2><p>This work was supported by JSPS KAKENHI Grant Number JP22H03642 and DENSO IT LAB Recognition and Learning Algorithm Collaborative Research Chair (Science Tokyo).</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://m82kada.github.io/portfolio/>Portfolio</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>